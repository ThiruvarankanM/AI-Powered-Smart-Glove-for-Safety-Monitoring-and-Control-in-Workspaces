{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLI5bKrGQwHGBBzFirqrAL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mathusayini/AI-Powered-Smart-Glove-for-Safety-Monitoring-Control-in-Workspaces/blob/main/SmartGloveManual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced Real-time Smart Glove Data Processing with Automatic ML"
      ],
      "metadata": {
        "id": "RaFm4x5THvBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import threading\n",
        "from threading import Lock\n",
        "import logging\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "import fcntl\n",
        "import contextlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "import warnings\n",
        "from firebase_admin import messaging\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import ssl\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "DErDhTpxHsab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('smartglove.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "_WLpQqJVJ9UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration class for the Smart Glove system"
      ],
      "metadata": {
        "id": "mBrdCAOKSpBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    csv_filename: str = \"smartglove_unified_dataset.csv\"\n",
        "    model_filename: str = \"smartglove_model.pkl\"\n",
        "    scaler_filename: str = \"smartglove_scaler.pkl\"\n",
        "    config_filename: str = \"smartglove_config.json\"\n",
        "\n",
        "    # Data retention settings\n",
        "    max_records: int = 50000\n",
        "    data_retention_days: int = 30\n",
        "\n",
        "    # ML settings\n",
        "    min_samples_for_training: int = 100\n",
        "    min_samples_for_retraining: int = 50\n",
        "    test_size: float = 0.2\n",
        "    random_state: int = 42\n",
        "\n",
        "    # Monitoring settings\n",
        "    monitoring_interval: int = 30\n",
        "    max_retries: int = 3\n",
        "    retry_delay: int = 5\n",
        "\n",
        "    # Firebase settings\n",
        "    firebase_db_url: str = 'https://emi-smartglove-default-rtdb.asia-southeast1.firebasedatabase.app'\n",
        "    firebase_paths: list = None\n",
        "\n",
        "    # Email alert settings\n",
        "    smtp_server: str = \"smtp.gmail.com\"\n",
        "    smtp_port: int = 587\n",
        "    sender_email: str = \"\"  # Your email\n",
        "    sender_password: str = \"\"  # Your app password\n",
        "    recipient_emails: list = None  # List of recipient emails\n",
        "\n",
        "    # FCM settings\n",
        "    fcm_server_key: str = \"\"  # Your FCM server key\n",
        "    fcm_topic: str = \"smartglove_alerts\"\n",
        "\n",
        "    # Alert thresholds\n",
        "    alert_confidence_threshold: float = 0.8\n",
        "    alert_predictions: list = None  # Predictions that trigger alerts\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.firebase_paths is None:\n",
        "            self.firebase_paths = ['smartglove/data', 'smartglove', 'data', 'smartglove/readings']\n",
        "        if self.recipient_emails is None:\n",
        "            self.recipient_emails = []\n",
        "        if self.alert_predictions is None:\n",
        "            self.alert_predictions = [\"emergency\", \"help\", \"danger\"]\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Save configuration to file\"\"\"\n",
        "        with open(self.config_filename, 'w') as f:\n",
        "            json.dump(asdict(self), f, indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls):\n",
        "        \"\"\"Load configuration from file\"\"\"\n",
        "        try:\n",
        "            with open(cls().config_filename, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                return cls(**data)\n",
        "        except FileNotFoundError:\n",
        "            logger.info(\"Config file not found, using defaults\")\n",
        "            return cls()"
      ],
      "metadata": {
        "id": "Sd5UBBYCKCZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class for handling alerts"
      ],
      "metadata": {
        "id": "NV1KxakFJwfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlertManager:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.last_alert_time = {}  # Track last alert time per prediction to avoid spam\n",
        "        self.alert_cooldown = 300  # 5 minutes cooldown between same alerts\n",
        "\n",
        "    def should_send_alert(self, prediction: str, confidence: float) -> bool:\n",
        "        \"\"\"Check if an alert should be sent based on prediction and confidence\"\"\"\n",
        "        if prediction.lower() not in [p.lower() for p in self.config.alert_predictions]:\n",
        "            return False\n",
        "\n",
        "        if confidence < self.config.alert_confidence_threshold:\n",
        "            return False\n",
        "\n",
        "        # Check cooldown\n",
        "        current_time = time.time()\n",
        "        last_alert = self.last_alert_time.get(prediction, 0)\n",
        "        if current_time - last_alert < self.alert_cooldown:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def send_fcm_notification(self, prediction: str, confidence: float):\n",
        "        \"\"\"Send push notification via FCM\"\"\"\n",
        "        try:\n",
        "            message = messaging.Message(\n",
        "                notification=messaging.Notification(\n",
        "                    title='Smart Glove Alert',\n",
        "                    body=f'Detection: {prediction.title()} (Confidence: {confidence:.2%})'\n",
        "                ),\n",
        "                topic=self.config.fcm_topic,\n",
        "                data={\n",
        "                    'prediction': prediction,\n",
        "                    'confidence': str(confidence),\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "            )\n",
        "\n",
        "            response = messaging.send(message)\n",
        "            logger.info(f\"üì± FCM notification sent: {response}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå FCM notification failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def send_email_alert(self, prediction: str, confidence: float):\n",
        "        \"\"\"Send email alert\"\"\"\n",
        "        try:\n",
        "            if not self.config.sender_email or not self.config.recipient_emails:\n",
        "                logger.warning(\"Email configuration incomplete\")\n",
        "                return False\n",
        "\n",
        "            # Create message\n",
        "            msg = MIMEMultipart()\n",
        "            msg['From'] = self.config.sender_email\n",
        "            msg['To'] = ', '.join(self.config.recipient_emails)\n",
        "            msg['Subject'] = f\"Smart Glove Alert: {prediction.title()}\"\n",
        "\n",
        "            # Email body\n",
        "            body = f\"\"\"\n",
        "            Smart Glove Alert Notification\n",
        "\n",
        "            Detection: {prediction.title()}\n",
        "            Confidence: {confidence:.2%}\n",
        "            Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "            This is an automated alert from your Smart Glove system.\n",
        "            Please check the device status if this is unexpected.\n",
        "\n",
        "            ---\n",
        "            Smart Glove ML Pipeline\n",
        "            \"\"\"\n",
        "\n",
        "            msg.attach(MIMEText(body, 'plain'))\n",
        "\n",
        "            # Send email\n",
        "            context = ssl.create_default_context()\n",
        "            with smtplib.SMTP(self.config.smtp_server, self.config.smtp_port) as server:\n",
        "                server.starttls(context=context)\n",
        "                server.login(self.config.sender_email, self.config.sender_password)\n",
        "                server.send_message(msg)\n",
        "\n",
        "            logger.info(f\"üìß Email alert sent to {len(self.config.recipient_emails)} recipients\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Email alert failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def send_alert(self, prediction: str, confidence: float):\n",
        "        \"\"\"Send both FCM and email alerts\"\"\"\n",
        "        if not self.should_send_alert(prediction, confidence):\n",
        "            return\n",
        "\n",
        "        logger.info(f\"üö® Sending alert for prediction: {prediction} (confidence: {confidence:.2%})\")\n",
        "\n",
        "        # Update last alert time\n",
        "        self.last_alert_time[prediction] = time.time()\n",
        "\n",
        "        # Send FCM notification\n",
        "        fcm_success = self.send_fcm_notification(prediction, confidence)\n",
        "\n",
        "        # Send email alert\n",
        "        email_success = self.send_email_alert(prediction, confidence)\n",
        "\n",
        "        # Log to Firebase for tracking\n",
        "        try:\n",
        "            alert_ref = db.reference('smartglove/alerts')\n",
        "            alert_data = {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'prediction': prediction,\n",
        "                'confidence': float(confidence),\n",
        "                'fcm_sent': fcm_success,\n",
        "                'email_sent': email_success\n",
        "            }\n",
        "            alert_ref.push(alert_data)\n",
        "            logger.info(f\"üìù Alert logged to Firebase: {alert_data}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ö†Ô∏è Failed to log alert to Firebase: {e}\")"
      ],
      "metadata": {
        "id": "rpCrPd46JrIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Validates sensor data quality and format"
      ],
      "metadata": {
        "id": "R0PFmnP7Stxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataValidator:\n",
        "    @staticmethod\n",
        "    def validate_sensor_data(df: pd.DataFrame) -> Tuple[bool, str]:\n",
        "        \"\"\"Validate sensor data format and ranges\"\"\"\n",
        "        if df.empty:\n",
        "            return False, \"Empty dataset\"\n",
        "\n",
        "        # Check required columns\n",
        "        required_cols = ['timestamp']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            return False, f\"Missing required columns: {missing_cols}\"\n",
        "\n",
        "        # Check for numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) < 2:  # At least timestamp and one sensor\n",
        "            return False, \"Insufficient numeric sensor data\"\n",
        "\n",
        "        # Check for reasonable sensor ranges (customize based on your sensors)\n",
        "        for col in numeric_cols:\n",
        "            if col == 'timestamp':\n",
        "                continue\n",
        "            if df[col].abs().max() > 10000:  # Adjust based on your sensor ranges\n",
        "                logger.warning(f\"Column {col} has values outside expected range\")\n",
        "\n",
        "        return True, \"Data validation passed\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_labels(labels: pd.Series) -> bool:\n",
        "        \"\"\"Validate label format and distribution\"\"\"\n",
        "        if labels.empty:\n",
        "            return False\n",
        "\n",
        "        # Check label distribution\n",
        "        label_counts = labels.value_counts()\n",
        "        if len(label_counts) < 2:\n",
        "            logger.warning(\"Only one label class found\")\n",
        "\n",
        "        # Check for minimum samples per class\n",
        "        min_samples = 10\n",
        "        insufficient_classes = label_counts[label_counts < min_samples]\n",
        "        if not insufficient_classes.empty:\n",
        "            logger.warning(f\"Classes with insufficient samples: {insufficient_classes.to_dict()}\")\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "id": "HGbBVYehKV2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thread-safe file operations with locking"
      ],
      "metadata": {
        "id": "Lb4RLpCwS1dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreadSafeFileHandler:\n",
        "    def __init__(self, filename: str):\n",
        "        self.filename = filename\n",
        "        self.lock = Lock()\n",
        "\n",
        "    @contextlib.contextmanager\n",
        "    def file_lock(self, mode='r'):\n",
        "        \"\"\"Context manager for file locking\"\"\"\n",
        "        with self.lock:\n",
        "            try:\n",
        "                f = open(self.filename, mode)\n",
        "                fcntl.flock(f.fileno(), fcntl.LOCK_EX)\n",
        "                yield f\n",
        "            finally:\n",
        "                fcntl.flock(f.fileno(), fcntl.LOCK_UN)\n",
        "                f.close()\n",
        "\n",
        "    def read_csv(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Thread-safe CSV reading\"\"\"\n",
        "        if not os.path.exists(self.filename):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with self.file_lock('r') as f:\n",
        "                return pd.read_csv(f)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading CSV: {e}\")\n",
        "            return None\n",
        "\n",
        "    def write_csv(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Thread-safe CSV writing\"\"\"\n",
        "        try:\n",
        "            with self.file_lock('w') as f:\n",
        "                df.to_csv(f, index=False)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error writing CSV: {e}\")\n",
        "            return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DKC4DVUhKSE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Firebase configuration with better error handling"
      ],
      "metadata": {
        "id": "5wz3I85LS6K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_firebase_config():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        colab_available = True\n",
        "    except ImportError:\n",
        "        colab_available = False\n",
        "\n",
        "    # Check existing JSON files\n",
        "    json_files = [f for f in os.listdir('.') if f.endswith('.json') and 'firebase' in f.lower()]\n",
        "\n",
        "    if json_files:\n",
        "        logger.info(f\"Found Firebase config: {json_files[0]}\")\n",
        "        return json_files[0]\n",
        "\n",
        "    if colab_available:\n",
        "        logger.info(\"Please upload your Firebase Admin SDK JSON file:\")\n",
        "        uploaded = files.upload()\n",
        "        return list(uploaded.keys())[0]\n",
        "    else:\n",
        "        raise RuntimeError(\"Place your Firebase Admin SDK JSON file in the working directory.\")\n"
      ],
      "metadata": {
        "id": "GNwn5KS-Kc1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Firebase with better error handling"
      ],
      "metadata": {
        "id": "U35JbXybS--G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_firebase(json_filename: str, db_url: str):\n",
        "    try:\n",
        "        if not os.path.exists(json_filename):\n",
        "            raise FileNotFoundError(f\"Firebase config file not found: {json_filename}\")\n",
        "\n",
        "        cred = credentials.Certificate(json_filename)\n",
        "        if not firebase_admin._apps:\n",
        "            firebase_admin.initialize_app(cred, {'databaseURL': db_url})\n",
        "            logger.info(\"Firebase initialized successfully\")\n",
        "        else:\n",
        "            logger.info(\"Firebase app already initialized\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Firebase initialization failed: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "z-CKzjNwKaUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced data processor with thread safety and validation"
      ],
      "metadata": {
        "id": "Huus2_uJTgAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmartGloveDataProcessor:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.file_handler = ThreadSafeFileHandler(config.csv_filename)\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.feature_columns = None\n",
        "        self.last_training_time = None\n",
        "        self.last_record_count = 0\n",
        "        self.validator = DataValidator()\n",
        "\n",
        "        # Load existing model if available\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load existing model and scaler\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.config.model_filename):\n",
        "                self.model = joblib.load(self.config.model_filename)\n",
        "                logger.info(\"Model loaded successfully\")\n",
        "\n",
        "            if os.path.exists(self.config.scaler_filename):\n",
        "                self.scaler = joblib.load(self.config.scaler_filename)\n",
        "                logger.info(\"Scaler loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {e}\")\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save trained model and scaler\"\"\"\n",
        "        try:\n",
        "            if self.model is not None:\n",
        "                joblib.dump(self.model, self.config.model_filename)\n",
        "            if self.scaler is not None:\n",
        "                joblib.dump(self.scaler, self.config.scaler_filename)\n",
        "            logger.info(\"Model and scaler saved successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving model: {e}\")\n",
        "\n",
        "    def find_data_path(self) -> Optional[str]:\n",
        "        for path in self.config.firebase_paths:\n",
        "            for attempt in range(self.config.max_retries):\n",
        "                try:\n",
        "                    ref = db.reference(path)\n",
        "                    data = ref.get()\n",
        "                    if data and isinstance(data, dict):\n",
        "                        if any(isinstance(v, dict) and len(v) > 1 for v in data.values()):\n",
        "                            logger.info(f\"Found data at path: '{path}'\")\n",
        "                            return path\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Attempt {attempt + 1} failed for path '{path}': {e}\")\n",
        "                    if attempt < self.config.max_retries - 1:\n",
        "                        time.sleep(self.config.retry_delay)\n",
        "\n",
        "        logger.error(\"No valid Firebase path found\")\n",
        "        return None\n",
        "\n",
        "    def fetch_and_process_data(self, data_path: str) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            ref = db.reference(data_path)\n",
        "            data = ref.get()\n",
        "            if not data:\n",
        "                logger.warning(\"No data found in Firebase\")\n",
        "                return None\n",
        "\n",
        "            ts_data = {}\n",
        "            for dtype, entries in data.items():\n",
        "                if not isinstance(entries, dict):\n",
        "                    continue\n",
        "                for ts, record in entries.items():\n",
        "                    if not isinstance(record, dict):\n",
        "                        continue\n",
        "                    if ts not in ts_data:\n",
        "                        ts_data[ts] = {}\n",
        "                    for k, v in record.items():\n",
        "                        if isinstance(v, dict):\n",
        "                            for sub_k, sub_v in v.items():\n",
        "                                ts_data[ts][f\"{k}_{sub_k}\"] = sub_v\n",
        "                        elif k != 'timestamp':\n",
        "                            ts_data[ts][k] = v\n",
        "\n",
        "            if not ts_data:\n",
        "                logger.warning(\"No timestamp data found\")\n",
        "                return None\n",
        "\n",
        "            df = pd.DataFrame.from_dict(ts_data, orient='index').reset_index()\n",
        "            df = df.rename(columns={'index': 'timestamp'})\n",
        "            df.columns = [c.replace(' ', '_').lower() for c in df.columns]\n",
        "\n",
        "            cols = ['timestamp'] + [c for c in df.columns if c not in ['timestamp', 'label']]\n",
        "            if 'label' in df.columns:\n",
        "                df['label'] = df['label'].astype(str).str.lower()\n",
        "                cols.append('label')\n",
        "            else:\n",
        "                df['label'] = 'unlabeled'\n",
        "                cols.append('label')\n",
        "\n",
        "            df = df[cols].sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "            is_valid, message = self.validator.validate_sensor_data(df)\n",
        "            if not is_valid:\n",
        "                logger.error(f\"Data validation failed: {message}\")\n",
        "                return None\n",
        "\n",
        "            logger.info(f\"Processed {len(df)} records from Firebase\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def update_csv(self, df: pd.DataFrame) -> bool:\n",
        "        if df is None or df.empty:\n",
        "            return False\n",
        "        try:\n",
        "            existing_df = self.file_handler.read_csv()\n",
        "            if existing_df is not None:\n",
        "                combined = pd.concat([existing_df, df], ignore_index=True)\n",
        "                combined = combined.drop_duplicates(subset=['timestamp'], keep='last')\n",
        "                combined = combined.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "                if len(combined) > self.config.max_records:\n",
        "                    combined = combined.tail(self.config.max_records)\n",
        "                    logger.info(f\"Applied record limit, keeping {len(combined)} records\")\n",
        "\n",
        "                if 'timestamp' in combined.columns:\n",
        "                    cutoff_time = datetime.now() - timedelta(days=self.config.data_retention_days)\n",
        "                    cutoff_timestamp = cutoff_time.timestamp()\n",
        "                    combined = combined[combined['timestamp'] >= cutoff_timestamp]\n",
        "                    logger.info(f\"Applied time retention, keeping {len(combined)} records\")\n",
        "            else:\n",
        "                combined = df\n",
        "\n",
        "            success = self.file_handler.write_csv(combined)\n",
        "            if success:\n",
        "                logger.info(f\"Updated CSV: {len(combined)} total records\")\n",
        "            return success\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating CSV: {e}\")\n",
        "            return False\n",
        "\n",
        "    def prepare_ml_data(self) -> Tuple[Optional[pd.DataFrame], Optional[pd.Series]]:\n",
        "        df = self.file_handler.read_csv()\n",
        "        if df is None:\n",
        "            return None, None\n",
        "\n",
        "        if 'label' not in df.columns:\n",
        "            logger.error(\"No label column found\")\n",
        "            return None, None\n",
        "\n",
        "        X = df.drop(columns=['timestamp', 'label'], errors='ignore')\n",
        "        X = X.select_dtypes(include=[np.number]).fillna(0)\n",
        "        y = df['label']\n",
        "\n",
        "        if not self.validator.validate_labels(y):\n",
        "            logger.warning(\"Label validation issues detected\")\n",
        "\n",
        "        self.feature_columns = X.columns.tolist()\n",
        "        logger.info(f\"Prepared ML data: {len(X)} samples, {len(X.columns)} features\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def should_retrain(self, current_sample_count: int) -> bool:\n",
        "        if self.model is None:\n",
        "            return current_sample_count >= self.config.min_samples_for_training\n",
        "\n",
        "        new_samples = current_sample_count - self.last_record_count\n",
        "        if new_samples < self.config.min_samples_for_retraining:\n",
        "            return False\n",
        "\n",
        "        if self.last_training_time is None:\n",
        "            return True\n",
        "\n",
        "        time_since_training = datetime.now() - self.last_training_time\n",
        "        return time_since_training.total_seconds() > 3600\n",
        "\n",
        "    def train_model(self, X: pd.DataFrame, y: pd.Series) -> bool:\n",
        "        if X.empty or y.empty:\n",
        "            logger.warning(\"Empty dataset for training\")\n",
        "            return False\n",
        "        try:\n",
        "            if len(X) < self.config.min_samples_for_training:\n",
        "                logger.warning(f\"Insufficient samples for training: {len(X)}\")\n",
        "                return False\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y,\n",
        "                test_size=self.config.test_size,\n",
        "                random_state=self.config.random_state,\n",
        "                stratify=y\n",
        "            )\n",
        "\n",
        "            self.scaler = StandardScaler()\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "            X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "            self.model = RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                random_state=self.config.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            self.model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            y_pred = self.model.predict(X_test_scaled)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            logger.info(f\"Model trained successfully | Accuracy: {accuracy:.4f}\")\n",
        "            logger.info(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "            self.save_model()\n",
        "\n",
        "            self.last_training_time = datetime.now()\n",
        "            self.last_record_count = len(X)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def predict_new_data(self, new_data) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
        "        if self.model is None or self.scaler is None:\n",
        "            logger.warning(\"Model not available for prediction\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            if isinstance(new_data, dict):\n",
        "                new_data = pd.DataFrame([new_data])\n",
        "\n",
        "            if self.feature_columns:\n",
        "                new_data = new_data.reindex(columns=self.feature_columns, fill_value=0)\n",
        "\n",
        "            new_data = new_data.fillna(0)\n",
        "            scaled = self.scaler.transform(new_data)\n",
        "            pred = self.model.predict(scaled)\n",
        "            proba = self.model.predict_proba(scaled)\n",
        "\n",
        "            return pred, proba\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error making prediction: {e}\")\n",
        "            return None, None\n"
      ],
      "metadata": {
        "id": "x-H350vznDiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced real-time monitoring with better error handling\n",
        "    "
      ],
      "metadata": {
        "id": "BNmCfD4AI2Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RealTimeMonitor:\n",
        "    def __init__(self, processor: SmartGloveDataProcessor, path: str, config: Config):\n",
        "        self.processor = processor\n",
        "        self.path = path\n",
        "        self.config = config\n",
        "        self.running = False\n",
        "        self.thread = None\n",
        "        self.error_count = 0\n",
        "        self.max_errors = 10\n",
        "        self.alert_manager = AlertManager(config)\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start monitoring with proper thread management\"\"\"\n",
        "        if self.running:\n",
        "            logger.warning(\"Monitor already running\")\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
        "        self.thread.start()\n",
        "        logger.info(f\"Real-time monitoring started (interval: {self.config.monitoring_interval}s)\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop monitoring gracefully\"\"\"\n",
        "        self.running = False\n",
        "        if self.thread and self.thread.is_alive():\n",
        "            self.thread.join(timeout=5)\n",
        "        logger.info(\"Monitoring stopped\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6jJPFf8qIy3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main monitoring loop with error recovery"
      ],
      "metadata": {
        "id": "nB2YAwaEInX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _monitor_loop(self):\n",
        "    while self.running:\n",
        "        try:\n",
        "            # Fetch new data\n",
        "            df = self.processor.fetch_and_process_data(self.path)\n",
        "\n",
        "            if df is not None and self.processor.update_csv(df):\n",
        "                # Prepare ML data\n",
        "                X, y = self.processor.prepare_ml_data()\n",
        "\n",
        "                # Check if retraining is needed\n",
        "                if X is not None and self.processor.should_retrain(len(X)):\n",
        "                    logger.info(\"Starting model retraining...\")\n",
        "                    if self.processor.train_model(X, y):\n",
        "                        logger.info(\"Model retraining completed\")\n",
        "                    else:\n",
        "                        logger.error(\"Model retraining failed\")\n",
        "\n",
        "                # Make prediction on latest data\n",
        "                if X is not None and not X.empty:\n",
        "                    latest_data = X.iloc[-1:].drop(columns=['label'], errors='ignore')\n",
        "                    pred, proba = self.processor.predict_new_data(latest_data)\n",
        "\n",
        "                    if pred is not None and proba is not None:\n",
        "                        confidence = max(proba[0])\n",
        "                        prediction = pred[0]\n",
        "\n",
        "                        logger.info(f\"Latest prediction: {prediction} (confidence: {confidence:.3f})\")\n",
        "\n",
        "                        # üî• Push prediction and confidence to Firebase\n",
        "                        try:\n",
        "                            prediction_ref = db.reference('smartglove/predictions')\n",
        "                            prediction_data = {\n",
        "                                'timestamp': datetime.now().isoformat(),\n",
        "                                'prediction': str(prediction),\n",
        "                                'confidence': float(confidence)\n",
        "                            }\n",
        "                            prediction_ref.push(prediction_data)\n",
        "                            logger.info(f\"üì§ Prediction pushed to Firebase: {prediction_data}\")\n",
        "                        except Exception as firebase_error:\n",
        "                            logger.error(f\"‚ö†Ô∏è Failed to push prediction to Firebase: {firebase_error}\")\n",
        "\n",
        "                        # üö® NEW: Check if alert should be sent\n",
        "                        self.alert_manager.send_alert(prediction, confidence)\n",
        "\n",
        "            # Reset error count on success\n",
        "            self.error_count = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.error_count += 1\n",
        "            logger.error(f\"Monitoring error ({self.error_count}/{self.max_errors}): {e}\")\n",
        "\n",
        "            if self.error_count >= self.max_errors:\n",
        "                logger.error(\"Too many errors, stopping monitor\")\n",
        "                self.running = False\n",
        "                break\n",
        "\n",
        "        # Sleep with early exit check\n",
        "        for _ in range(self.config.monitoring_interval):\n",
        "            if not self.running:\n",
        "                break\n",
        "            time.sleep(1)\n"
      ],
      "metadata": {
        "id": "BbSRK4VCIkTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_email_config():\n",
        "    \"\"\"Interactive setup for email configuration\"\"\"\n",
        "    print(\"\\n=== Email Alert Configuration ===\")\n",
        "    sender_email = input(\"Enter sender email (Gmail): \")\n",
        "    sender_password = input(\"Enter app password (not regular password): \")\n",
        "\n",
        "    recipients = []\n",
        "    while True:\n",
        "        email = input(\"Enter recipient email (or press Enter to finish): \")\n",
        "        if not email:\n",
        "            break\n",
        "        recipients.append(email)\n",
        "\n",
        "    return sender_email, sender_password, recipients"
      ],
      "metadata": {
        "id": "ml5jNFg5K2OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_fcm_topic():\n",
        "    \"\"\"Setup FCM topic for notifications\"\"\"\n",
        "    try:\n",
        "        # This would typically be done on the client side (mobile app)\n",
        "        # For now, we'll just log the topic name\n",
        "        logger.info(\"üì± FCM Topic: smartglove_alerts\")\n",
        "        logger.info(\"Subscribe your mobile app to this topic to receive notifications\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"FCM setup failed: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "6InDD_viK_3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_firebase_config():\n",
        "    \"\"\"Upload Firebase Admin SDK JSON file in Google Colab\"\"\"\n",
        "    from google.colab import files\n",
        "    logger.info(\"Please upload your Firebase Admin SDK JSON file now:\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.json'):\n",
        "            logger.info(f\"Uploaded file: {filename}\")\n",
        "            return filename\n",
        "    raise RuntimeError(\"No JSON file uploaded. Please upload a valid Firebase Admin SDK JSON file.\")\n"
      ],
      "metadata": {
        "id": "OBwstvGiRU_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function with enhanced error handling"
      ],
      "metadata": {
        "id": "oTdXjkpQINcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    logger.info(\"=== ENHANCED SMART GLOVE REAL-TIME ML PIPELINE ===\")\n",
        "    try:\n",
        "        config = Config.load()\n",
        "        config.save()\n",
        "\n",
        "        # Upload Firebase Admin SDK JSON file interactively in Colab\n",
        "        json_file = upload_firebase_config()\n",
        "\n",
        "        if not config.sender_email or not config.recipient_emails:\n",
        "            sender_email, sender_password, recipients = setup_email_config()\n",
        "            config.sender_email = sender_email\n",
        "            config.sender_password = sender_password\n",
        "            config.recipient_emails = recipients\n",
        "            config.save()\n",
        "\n",
        "        # Setup FCM\n",
        "        setup_fcm_topic()\n",
        "\n",
        "        if not initialize_firebase(json_file, config.firebase_db_url):\n",
        "            logger.error(\"‚ùå Failed to initialize Firebase\")\n",
        "            return\n",
        "\n",
        "        processor = SmartGloveDataProcessor(config)\n",
        "        path = processor.find_data_path()\n",
        "        if not path:\n",
        "            logger.error(\"‚ùå No valid Firebase path found\")\n",
        "            return\n",
        "\n",
        "        logger.info(\"üì¶ Fetching initial data...\")\n",
        "        df = processor.fetch_and_process_data(path)\n",
        "        if df is not None:\n",
        "            processor.update_csv(df)\n",
        "            X, y = processor.prepare_ml_data()\n",
        "            if X is not None and len(X) >= config.min_samples_for_training:\n",
        "                logger.info(\"üîÅ Training initial model...\")\n",
        "                processor.train_model(X, y)\n",
        "\n",
        "        monitor = RealTimeMonitor(processor, path, config)\n",
        "        monitor.start()\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                time.sleep(10)\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"üõë Shutdown requested by user.\")\n",
        "        finally:\n",
        "            monitor.stop()\n",
        "            logger.info(\"‚úÖ Smart Glove ML pipeline shutdown complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"‚ùå Fatal error in main: {e}\")\n"
      ],
      "metadata": {
        "id": "BNeFY417m_t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "jYMkBIXNRus3",
        "outputId": "8087279f-f31a-44b1-d4e1-14c8089735a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a823a650-6c88-4cf5-b9fb-fb172a2805bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a823a650-6c88-4cf5-b9fb-fb172a2805bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving emi-smartglove-firebase-adminsdk-fbsvc-7ce1c7583b.json to emi-smartglove-firebase-adminsdk-fbsvc-7ce1c7583b (2).json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:No valid Firebase path found\n",
            "ERROR:__main__:‚ùå No valid Firebase path found\n"
          ]
        }
      ]
    }
  ]
}